{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ou25OS6_IF3"
   },
   "source": [
    "# Coursework: Mammographic density prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PU4OW80_IF6"
   },
   "source": [
    "In this coursework, you will implement a model for breast tissue density prediction from digital screening mammography data. Mammograms are X‑ray images acquired as part of regular breast cancer screening programmes. Breast tissue density is an important risk factor and is categorised into four classes according to the BI‑RADS (Breast Imaging Reporting and Data System) standard:\n",
    "- Class A – Almost entirely fatty: The breast is composed mostly of fatty tissue with minimal fibroglandular density.\n",
    "- Class B – Scattered areas of fibroglandular density: There are some dense areas, but most of the tissue is still fatty.\n",
    "- Class C – Heterogeneously dense: Many areas of dense tissue are present, which may obscure small masses.\n",
    "- Class D – Extremely dense: The breast is composed predominantly of dense tissue, making mammographic interpretation and cancer detection more challenging.\n",
    "\n",
    "Your task is to develop and evaluate a predictive model capable of automatically assigning these density categories from mammographic images. You will be asked to assess the performance of the model across different subgroups, and conduct a model inspection using dimensionality reduction of feature embeddings.\n",
    "\n",
    "The coursework is divided in the following parts:\n",
    "\n",
    "* **Part A**: Choose a sensible data augmentation pipeline within the `MammoDataset` class.\n",
    "* **Part B**: Implement a sensible model for image classification within the `MammoNet` class.\n",
    "* **Part C**: Conduct a subgroup performance analysis.\n",
    "* **Part D**: Inspect the trained model by analysing feature embeddings.\n",
    "* **Part E**: Write a short report about your coursework (using the [provided template](https://www.overleaf.com/read/rkjxjntdfdwr#df7c9e)).\n",
    "\n",
    "**Important:** Read the text descriptions carefully and look out for hints and comments indicating a specific **TASK**. Make sure to add sufficient documentation and comments to your code.\n",
    "\n",
    "**Submission:** You are asked to submit two files:\n",
    "1. You should submit this notebook in `.ipynb` format with *all outputs included*. Please name your file `notebook.ipynb`.\n",
    "2. You should also submit a short report in `.pdf` format, using [this template](https://www.overleaf.com/read/rkjxjntdfdwr#df7c9e). Please name this file `report.pdf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your details\n",
    "\n",
    "Please add your details below. You can work in groups up to two.\n",
    "\n",
    "Authors: **firstname1 lastname1** & **firstname2 lastname2**\n",
    "\n",
    "DoC username: **alias1** & **alias2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GbHdKTKx_IF6"
   },
   "outputs": [],
   "source": [
    "# On Google Colab uncomment the following line to install additional libraries\n",
    "# ! pip install lightning\n",
    "# ! pip install stocaching\n",
    "\n",
    "# See the requirements.txt file for more details on the required libraries.\n",
    "# We recommend using a virtual environment to manage dependencies.\n",
    "# For example, you can create a virtual environment using venv:\n",
    "# python3 -m venv myenv\n",
    "# source myenv/bin/activate\n",
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqBURSbj_IF7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as T\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import cm\n",
    "from ipywidgets import Output, HBox\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn import decomposition\n",
    "from sklearn.manifold import TSNE\n",
    "from skimage.io import imread\n",
    "from skimage.util import img_as_ubyte\n",
    "from torchmetrics.functional import auroc\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "from stocaching import SharedCache\n",
    "from torchsampler import ImbalancedDatasetSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAFbnkol_IF7"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make use of the publicly available [RSNA-SMBC dataset](https://registry.opendata.aws/rsna-screening-mammography-breast-cancer-detection/). We have preproceesed this dataset to make it easier to work with. We have reduced the image sizes to 128x96 and simplified the meta information.\n",
    "\n",
    "Note, unzipping the dataset will take a few minutes, as there almost 30,000 images. Only needs to be done once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6bxuGFp_IF8"
   },
   "outputs": [],
   "source": [
    "! wget https://www.doc.ic.ac.uk/~bglocker/teaching/mli/rsna-small.zip\n",
    "! unzip -q rsna-small.zip # Remove -q for verbose output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/rsna-small'\n",
    "CSV_FILE = os.path.join(DATA_DIR, 'meta.csv')\n",
    "IMAGE_SIZE = (128, 96)\n",
    "NUM_CLASSES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the meta information of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv(CSV_FILE)\n",
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0V6TlWwW_IF8"
   },
   "source": [
    "Let's visualise some random samples from each density class. You can run this cell repeatedly to see different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_DpVMTv_IF8"
   },
   "outputs": [],
   "source": [
    "N_PER_CLASS = 4  # Number of images per class to display\n",
    "CLASSES_DISPLAY = [\"A\", \"B\", \"C\", \"D\"] # Breast density classes for display\n",
    "\n",
    "df_meta[\"img_path\"] = [\n",
    "    os.path.join(\n",
    "        DATA_DIR, \"images\",\n",
    "        str(df_meta.study_id.values[i]),\n",
    "        str(df_meta.image_id.values[i]) + \".png\"\n",
    "    )\n",
    "    for i in range(len(df_meta))\n",
    "]\n",
    "\n",
    "# display sample images from each class\n",
    "fig, axes = plt.subplots(len(CLASSES_DISPLAY), N_PER_CLASS, figsize=(1.9 * N_PER_CLASS, 2.2 * len(CLASSES_DISPLAY)))\n",
    "if N_PER_CLASS == 1: # In case of single column, expand dims\n",
    "    axes = np.expand_dims(axes, axis=1)\n",
    "\n",
    "for r, cls in enumerate(CLASSES_DISPLAY):\n",
    "    sub = df_meta[df_meta[\"density\"] == cls]\n",
    "    k = min(N_PER_CLASS, len(sub))\n",
    "    sample_idx = np.random.choice(len(sub), size=k, replace=False)\n",
    "    samples = sub.iloc[sample_idx]\n",
    "    for c in range(N_PER_CLASS):\n",
    "        ax = axes[r, c]\n",
    "        ax.axis(\"off\")\n",
    "        if c < k:\n",
    "            img = imread(samples.iloc[c][\"img_path\"])\n",
    "            ax.imshow(img, cmap=\"gray\")\n",
    "            ax.set_title(f\"Density {cls}\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part A**: Choose a sensible data augmentation pipeline within the `MammoDataset` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90Hi9h0g_IF8"
   },
   "source": [
    "We provide a custom [PyTorch Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) that manages the RSNA-SMBC dataset.\n",
    "\n",
    "**TASK:** Implement data augmentation for mammography images. You can use the `torchvision.transforms.v2` module to select a sensible set of photometric and geometric transformations. Report your choices in the coursework report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Btoqyv_E_IF9"
   },
   "outputs": [],
   "source": [
    "class MammoDataset(Dataset):\n",
    "    def __init__(self, data, image_size, image_normalization, augmentation = False, cache_size = 0):\n",
    "        self.image_normalization = image_normalization\n",
    "        self.do_augment = augmentation\n",
    "\n",
    "        # photometric data augmentation\n",
    "        self.photometric_augment = T.Compose([\n",
    "            T.RandomApply(transforms=[T.Grayscale()], p=0.5)\n",
    "        ])\n",
    "\n",
    "        # geometric data augmentation\n",
    "        self.geometric_augment = T.Compose([\n",
    "            T.RandomApply(transforms=[T.Resize(size=image_size)], p=0.5)\n",
    "        ])\n",
    "\n",
    "        # load metadata into numpy arrays for faster access\n",
    "        self.img_paths = data.img_path.to_numpy()\n",
    "        self.study_ids = data.study_id.to_numpy()\n",
    "        self.image_ids = data.image_id.to_numpy()\n",
    "        self.labels = data.labels.to_numpy()\n",
    "\n",
    "        # cache for preprocessed images to speed up training after the first epoch\n",
    "        self.cache = None\n",
    "        self.use_cache = cache_size > 0\n",
    "        if self.use_cache:\n",
    "            self.cache = SharedCache(\n",
    "                size_limit_gib=cache_size,\n",
    "                dataset_len=self.labels.shape[0],\n",
    "                data_dims=(1, image_size[0], image_size[1]),\n",
    "                dtype=torch.float32,\n",
    "            )\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        # breast segmentation using connected components\n",
    "        image_norm = image - np.min(image)\n",
    "        image_norm = image_norm / np.max(image_norm)\n",
    "        thresh = cv2.threshold(img_as_ubyte(image_norm), 5, 255, cv2.THRESH_BINARY)[1]\n",
    "        nb_components, output, stats, _ = cv2.connectedComponentsWithStats(thresh, connectivity=4)\n",
    "\n",
    "        # Find the largest non background component.\n",
    "        max_label, _ = max(\n",
    "            [(i, stats[i, cv2.CC_STAT_AREA]) for i in range(1, nb_components)],\n",
    "            key=lambda x: x[1],\n",
    "        )\n",
    "        mask = output == max_label\n",
    "        image[mask == 0] = 0\n",
    "                \n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = None\n",
    "        if self.use_cache:\n",
    "            image = self.cache.get_slot(index)\n",
    "        \n",
    "        if image is None:\n",
    "            img_path = self.img_paths[index]\n",
    "            image = imread(img_path).astype(np.float32)\n",
    "            image = self.preprocess(image)\n",
    "            image = torch.from_numpy(image).unsqueeze(0)            \n",
    "            \n",
    "            if self.use_cache:\n",
    "                self.cache.set_slot(index, image, allow_overwrite=True)\n",
    "\n",
    "        # normalize intensities to range [0,1]\n",
    "        image = image / self.image_normalization\n",
    "\n",
    "        if self.do_augment:\n",
    "            image = self.photometric_augment(image)\n",
    "            image = self.geometric_augment(image)\n",
    "\n",
    "        # convert to 3-channel image to be compatible with pretrained models\n",
    "        image = image.repeat(3, 1, 1)\n",
    "\n",
    "        return {'image': image, 'label': self.labels[index], 'study_id': self.study_ids[index], 'image_id': self.image_ids[index]}\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return self.labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWnGp601_IF9"
   },
   "source": [
    "We use a [LightningDataModule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html) for preparing the RSNA-SMBC dataset and its training, validation and test splits. No changes required here but make sure that you understand what is happening within this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxWMcxlh_IF9"
   },
   "outputs": [],
   "source": [
    "class MammoDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, csv_file, image_size, batch_size, num_workers):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        # load metadata\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # construct image paths from metadata\n",
    "        self.data['img_path'] = [os.path.join(self.data_dir, 'images', str(self.data.study_id.values[idx]), str(self.data.image_id.values[idx]) + '.png') for idx in range(0, len(self.data))]\n",
    "\n",
    "        # Define image labels based on breast density classes\n",
    "        self.data['labels'] = self.data['density']\n",
    "        self.data.loc[self.data['labels'] == 'A', 'labels'] = 0\n",
    "        self.data.loc[self.data['labels'] == 'B', 'labels'] = 1\n",
    "        self.data.loc[self.data['labels'] == 'C', 'labels'] = 2\n",
    "        self.data.loc[self.data['labels'] == 'D', 'labels'] = 3\n",
    "\n",
    "        # Use pre-defined splits to separate data into train, val and testing\n",
    "        self.train_data = self.data[self.data['split'] == 'training']\n",
    "        self.val_data = self.data[self.data['split'] == 'validation']\n",
    "        self.test_data = self.data[self.data['split'] == 'test']\n",
    "\n",
    "        # create dataset objects for each split\n",
    "        self.train_set = MammoDataset(self.train_data, self.image_size, image_normalization=65535.0, augmentation=True, cache_size=2)\n",
    "        self.val_set = MammoDataset(self.val_data, self.image_size, image_normalization=65535.0, augmentation=False, cache_size=0.5)\n",
    "        self.test_set = MammoDataset(self.test_data, self.image_size, image_normalization=65535.0, augmentation=False)\n",
    "\n",
    "        # print dataset statistics\n",
    "        train_labels = self.train_set.get_labels()\n",
    "        train_class_count = np.array([len(np.where(train_labels == t)[0]) for t in np.unique(train_labels)])\n",
    "\n",
    "        val_labels = self.val_set.get_labels()        \n",
    "        val_class_count = np.array([len(np.where(val_labels == t)[0]) for t in np.unique(val_labels)])\n",
    "\n",
    "        test_labels = self.test_set.get_labels()        \n",
    "        test_class_count = np.array([len(np.where(test_labels == t)[0]) for t in np.unique(test_labels)])\n",
    "\n",
    "        print('samples (train): ',len(self.train_set))\n",
    "        print('samples (val):   ',len(self.val_set))\n",
    "        print('samples (test):  ',len(self.test_set))\n",
    "        print('class counts (train): ', train_class_count)\n",
    "        print('class counts (val):   ', val_class_count)\n",
    "        print('class counts (test):  ', test_class_count)\n",
    "        print('class % (train): ', np.array([f\"{x:.2f}\" for x in np.array(train_class_count/len(train_labels)*100.0)]))\n",
    "        print('class % (val):   ', np.array([f\"{x:.2f}\" for x in np.array(val_class_count/len(val_labels)*100.0)]))\n",
    "        print('class % (test):  ', np.array([f\"{x:.2f}\" for x in np.array(test_class_count/len(test_labels)*100.0)]))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # we use ImbalancedDatasetSampler to handle class imbalance in the training set\n",
    "        return DataLoader(dataset=self.train_set, batch_size=self.batch_size, sampler=ImbalancedDatasetSampler(self.train_set), num_workers=self.num_workers, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(dataset=self.val_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(dataset=self.test_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9LNMrvq_IF9"
   },
   "source": [
    "## **Part B**: Implement a sensible model for image classification within the `MammoNet` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "044c3c3-_IF9"
   },
   "source": [
    "We use a [LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html) for implementing the model and its training and testing steps.\n",
    "\n",
    "**TASK:** Implement and try out different models suitable for the breast density prediction problem. Check out the `torchvision.models` module. Report your choices in the coursework report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJbVvqx7_IF9"
   },
   "outputs": [],
   "source": [
    "class MammoNet(pl.LightningModule):\n",
    "    def __init__(self, num_classes, learning_rate=0.0001):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # naive two-layer MLP model\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(torch.prod(torch.tensor(IMAGE_SIZE)) * 3, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # flatten the input image\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def process_batch(self, batch):\n",
    "        img, lab = batch['image'], batch['label']\n",
    "        out = self.forward(img)\n",
    "        prd = torch.softmax(out, dim=1)\n",
    "        loss = F.cross_entropy(out, lab)\n",
    "        return loss, prd, lab\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.train_preds = []\n",
    "        self.train_trgts = []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, prd, lab = self.process_batch(batch)\n",
    "        self.log('train_loss', loss, batch_size=lab.shape[0])\n",
    "        self.train_preds.append(prd.detach().cpu())\n",
    "        self.train_trgts.append(lab.detach().cpu())\n",
    "        if batch_idx == 0:\n",
    "            images = batch['image'][0:4, ...].detach().cpu()\n",
    "            grid = torchvision.utils.make_grid(images, nrow=2, normalize=True)\n",
    "            self.logger.experiment.add_image('images', grid, self.global_step)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.train_preds = torch.cat(self.train_preds, dim=0)\n",
    "        self.train_trgts = torch.cat(self.train_trgts, dim=0)\n",
    "        auc = auroc(self.train_preds, self.train_trgts, num_classes=self.num_classes, average='macro', task='multiclass')\n",
    "        self.log('train_auc', auc)\n",
    "        self.train_preds = []\n",
    "        self.train_trgts = []\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self.val_preds = []\n",
    "        self.val_trgts = []\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, prd, lab = self.process_batch(batch)\n",
    "        self.log('val_loss', loss, batch_size=lab.shape[0])\n",
    "        self.val_preds.append(prd.detach().cpu())\n",
    "        self.val_trgts.append(lab.detach().cpu())\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.val_preds = torch.cat(self.val_preds, dim=0)\n",
    "        self.val_trgts = torch.cat(self.val_trgts, dim=0)\n",
    "        auc = auroc(self.val_preds, self.val_trgts, num_classes=self.num_classes, average='macro', task='multiclass')\n",
    "        self.log('val_auc', auc)\n",
    "        self.val_preds = []\n",
    "        self.val_trgts = []\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.test_preds = []\n",
    "        self.test_trgts = []\n",
    "        self.test_study_ids = []\n",
    "        self.test_image_ids = []\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, prd, lab = self.process_batch(batch)\n",
    "        self.log('test_loss', loss, batch_size=lab.shape[0])\n",
    "        self.test_preds.append(prd.detach().cpu())\n",
    "        self.test_trgts.append(lab.detach().cpu())        \n",
    "        self.test_study_ids.append(batch['study_id'].detach().cpu())\n",
    "        self.test_image_ids.append(batch['image_id'].detach().cpu())\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.test_preds = torch.cat(self.test_preds, dim=0)\n",
    "        self.test_trgts = torch.cat(self.test_trgts, dim=0)\n",
    "        auc = auroc(self.test_preds, self.test_trgts, num_classes=self.num_classes, average='macro', task='multiclass')\n",
    "        self.log('test_auc', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtNkaTZQ_IF-"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nI3Kj1hA_IF-"
   },
   "source": [
    "We use the PyTorch Lightning [Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html) for easy training and testing. You may need to adjust the number of epochs, but otherwise you should be able to just run the cell and wait for the training to complete.\n",
    "\n",
    "**TASK:** Adjust the number of epochs if needed. Report any observations about the training in your coursework report (you may want to check training and validation curves via Tensorboard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t54MjA_x_IF-"
   },
   "outputs": [],
   "source": [
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "output_base = './output/'\n",
    "output_name = 'mammo-net'\n",
    "output_dir = os.path.join(output_base, output_name)\n",
    "\n",
    "data = MammoDataModule(data_dir=DATA_DIR, csv_file=CSV_FILE, image_size=IMAGE_SIZE, batch_size=256, num_workers=4)\n",
    "\n",
    "model = MammoNet(num_classes=NUM_CLASSES, learning_rate=0.001)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=2,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    log_every_n_steps=5,\n",
    "    logger=TensorBoardLogger(save_dir=output_base, name=output_name),\n",
    "    callbacks=[ModelCheckpoint(monitor=\"val_auc\", mode='max'), TQDMProgressBar(refresh_rate=10)],\n",
    ")\n",
    "trainer.fit(model=model, datamodule=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgGetYUu_IF-"
   },
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wQpwwV-_IF-"
   },
   "source": [
    "Evaluate the trained model with the best checkpoint on the validation data and report the classification performance.\n",
    "\n",
    "**TASK:** You should report the validation performance and some information about the model checkpoint in the coursework report (at what epoch/iteration did you find the best checkpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tKhRhBp_IF-"
   },
   "outputs": [],
   "source": [
    "trainer.validate(model=model, datamodule=data, ckpt_path=trainer.checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqfQleYN_IF-"
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLmxhx2y_IF_"
   },
   "source": [
    "Evaluate the trained model with the best checkpoint on the test data and report the classification performance.\n",
    "\n",
    "**TASK:** You should report the test performance in the coursework report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(model, output_fname):\n",
    "    std_ids = [id.numpy() for sublist in model.test_study_ids for id in sublist]\n",
    "    img_ids = [id.numpy() for sublist in model.test_image_ids for id in sublist]\n",
    "    cols_names = ['class_' + str(i) for i in range(0, NUM_CLASSES)]\n",
    "    df = pd.DataFrame(data=model.test_preds.numpy(), columns=cols_names)    \n",
    "    df['target'] = model.test_trgts.numpy()\n",
    "    df['study_id'] = std_ids\n",
    "    df['image_id'] = img_ids\n",
    "    df.to_csv(output_fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSeq_7q6_IF_"
   },
   "outputs": [],
   "source": [
    "trainer.test(model=model, datamodule=data, ckpt_path=trainer.checkpoint_callback.best_model_path)\n",
    "save_predictions(model=model, output_fname=os.path.join(output_dir, 'predictions.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part C**: Conduct a subgroup performance analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8GUR4qk_IF_"
   },
   "source": [
    "The code below allows you to check the model performance by plotting the [ROC curves](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) and calculating the per-class [AUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html) metrics over the whole test set.\n",
    "\n",
    "**TASK:** Using different attributes from the meta information, conduct several subgroup performance analyses, and report the results in the coursework report. Note any interesting observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoVg1pqR_IF_"
   },
   "outputs": [],
   "source": [
    "df_pred = pd.read_csv(os.path.join(output_dir, 'predictions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_columns = [col for col in df_pred.columns if col.startswith('class_')]\n",
    "preds = np.stack([df_pred[col] for col in class_columns]).T\n",
    "targets = np.array(df_pred['target'])\n",
    "\n",
    "roc_results = {}\n",
    "\n",
    "for i, class_name in enumerate(class_columns):\n",
    "    pos_label = i\n",
    "    y = np.array(targets)\n",
    "    y[targets != pos_label] = 0\n",
    "    y[targets == pos_label] = 1\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y, preds[:, pos_label])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    roc_results[class_name] = {\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'roc_auc': roc_auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "for i, (class_name, metrics) in enumerate(roc_results.items()):\n",
    "    label = f\"Class {i} AUC={metrics['roc_auc']:.2f}\"\n",
    "    plt.plot(metrics['fpr'], metrics['tpr'], lw=1.5, alpha=.8, label=label)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=1.5, color='k', label='Chance', alpha=.8)\n",
    "plt.xlabel('False Positive Rate', fontsize=14)\n",
    "plt.ylabel('True Positive Rate', fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.title('Classification Performance', fontsize=14)\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part D**: Inspect the trained model by analysing feature embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract embeddings\n",
    "\n",
    "We first need to extract the feature embeddings from the classification model.\n",
    "\n",
    "**TASK:** Adapt the code below and make it work for your classification model. You only need to adapt the functions `on_test_epoch_start` and `test_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derived class to extract embeddings for visualization in addition to predictions\n",
    "class MammoNetEmbeddings(MammoNet):\n",
    "    def __init__(self, num_classes, learning_rate=0.0001):\n",
    "        super().__init__(num_classes, learning_rate)\n",
    "        self.embeddings = [] # list where we still store the embeddings\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.model[2] = nn.Identity(512) # replace final classification layer with identity to get embeddings\n",
    "        self.embeddings = [] # clear the list of embeddings at the start of testing\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch['image'].view(batch['image'].size(0), -1)\n",
    "        emb = self.model(x)\n",
    "        self.embeddings.append(emb.detach().cpu())\n",
    "\n",
    "    # Override to disable other test logging of base class\n",
    "    def on_test_epoch_end(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(model, output_fname):\n",
    "    embeddings = torch.cat(model.embeddings, dim=0).cpu().numpy()\n",
    "    df = pd.DataFrame(data=embeddings)\n",
    "    df.to_csv(output_fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emb = MammoNetEmbeddings(num_classes=NUM_CLASSES)\n",
    "trainer.test(model=model_emb, datamodule=data, ckpt_path=trainer.checkpoint_callback.best_model_path)\n",
    "save_embeddings(model=model_emb, output_fname=os.path.join(output_dir, 'embeddings.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model inspection\n",
    "\n",
    "Now that we extracted the feature embeddings for the test set, we can combine this information with the meta data and the model predictions.\n",
    "\n",
    "**TASK:** Adapt the provided code to conduct a model inspection, using different projections from different dimensionality reduction techniques, and overlay different types of meta information. Report the your findings in the coursework report. Note any interesting observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv(CSV_FILE)\n",
    "df_pred = pd.read_csv(os.path.join(output_dir, 'predictions.csv'))\n",
    "df = pd.merge(df_pred, df_meta, how='inner', on=['image_id', 'study_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.read_csv(os.path.join(output_dir, 'embeddings.csv')).to_numpy()\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run PCA to reduce the dimenionality of the feature embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=4, whiten=False)\n",
    "embeddings_pca = pca.fit_transform(embeddings)\n",
    "\n",
    "print(embeddings_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pca_1'] = embeddings_pca[:,0]\n",
    "df['pca_2'] = embeddings_pca[:,1]\n",
    "df['pca_3'] = embeddings_pca[:,2]\n",
    "df['pca_4'] = embeddings_pca[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_tsne = TSNE(n_components=2, init='random', learning_rate='auto').fit_transform(embeddings_pca)\n",
    "\n",
    "print(embeddings_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tsne_1'] = embeddings_tsne[:,0]\n",
    "df['tsne_2'] = embeddings_tsne[:,1]\n",
    "\n",
    "df.head() # showing the first five entries in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shuffle the dataframe to ensure random order of samples for visualization\n",
    "df = df.sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.6\n",
    "style = '.'\n",
    "markersize = 40\n",
    "color_palette = 'tab10'\n",
    "kind = 'scatter'\n",
    "\n",
    "def plot_scatter(data, hue, x, y, palette):\n",
    "    hue_order = list(data[hue].unique())\n",
    "    hue_order.sort()\n",
    "    sns.set_theme(style=\"white\")\n",
    "    ax = sns.scatterplot(data=data, x=x, y=y, hue=hue, hue_order=hue_order, alpha=alpha, marker=style, s=markersize, palette=palette)\n",
    "    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "def plot_joint(data, hue, x, y, palette):\n",
    "    hue_order = list(data[hue].unique())\n",
    "    hue_order.sort()\n",
    "    sns.set_theme(style=\"white\")\n",
    "    ax = sns.jointplot(data=data, x=x, y=y, hue=hue, hue_order=hue_order, alpha=alpha, marker=style, s=markersize, palette=palette, marginal_kws={'common_norm': False})\n",
    "    sns.move_legend(ax.ax_joint, \"upper left\", bbox_to_anchor=(1.2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'pca_1'\n",
    "y = 'pca_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint(df, 'density', x, y, color_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-y_bpDEZ_IF_"
   },
   "source": [
    "### Interactive visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the code below you can interactively explore the latent space to better understand how samples are distributed. You can modify the code to overlay different types of meta information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"image_path\"] = [\n",
    "    os.path.join(\"images\", str(df.study_id.values[idx]), str(df.image_id.values[idx]) + \".png\")\n",
    "    for idx in range(len(df))\n",
    "]\n",
    "\n",
    "df['color_label'] = 0\n",
    "df.loc[df['density'] == 'A', 'color_label'] = 0\n",
    "df.loc[df['density'] == 'B', 'color_label'] = 1\n",
    "df.loc[df['density'] == 'C', 'color_label'] = 2\n",
    "df.loc[df['density'] == 'D', 'color_label'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_hex(rgb):\n",
    "    return '#{:02x}{:02x}{:02x}'.format(rgb[0], rgb[1], rgb[2])\n",
    "\n",
    "color = cm.tab10(np.linspace(0, 1, 10))\n",
    "colorlist = [(np.array(mpl.colors.to_rgb(c))*255).astype(int).tolist() for c in color]*10\n",
    "\n",
    "colors = [rgb_to_hex(colorlist[c]) for c in df.color_label.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    # breast mask\n",
    "    image_norm = image - np.min(image)\n",
    "    image_norm = image_norm / np.max(image_norm)\n",
    "    thresh = cv2.threshold(img_as_ubyte(image_norm), 5, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # Connected components with stats.\n",
    "    nb_components, output, stats, _ = cv2.connectedComponentsWithStats(thresh, connectivity=4)\n",
    "\n",
    "    # Find the largest non background component.\n",
    "    # Note: range() starts from 1 since 0 is the background label.\n",
    "    max_label, _ = max(\n",
    "        [(i, stats[i, cv2.CC_STAT_AREA]) for i in range(1, nb_components)],\n",
    "        key=lambda x: x[1],\n",
    "    )\n",
    "    mask = output == max_label\n",
    "    image_masked = image.copy()\n",
    "    image_masked[mask == 0] = 0\n",
    "\n",
    "    return image_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Output()\n",
    "@out.capture(clear_output=True)\n",
    "def handle_click(trace, points, state):\n",
    "    sample = df.iloc[points.point_inds[0]]\n",
    "    img_orig = imread(os.path.join(DATA_DIR, sample.image_path))\n",
    "    img_proc = preprocess(img_orig)\n",
    "    \n",
    "    s = [6] * len(df)\n",
    "    for i in points.point_inds:\n",
    "        s[i] = 12\n",
    "    with fig.batch_update():\n",
    "        scatter.marker.size = s\n",
    "\n",
    "    f, (ax1, ax2) = plt.subplots(1,2, figsize=(8,8))\n",
    "    ax1.imshow(img_orig, cmap='gray')\n",
    "    ax1.set_title('original')\n",
    "    ax1.axis('off')\n",
    "    ax2.imshow(img_proc, cmap='gray')\n",
    "    ax2.set_title('processed')\n",
    "    ax2.axis('off')\n",
    "    plt.show(f)\n",
    "    \n",
    "fig = go.FigureWidget(px.scatter(df, x=x, y=y, template='simple_white', hover_data={'density': True, x:False, y:False}))\n",
    "fig.update_layout(width=600, height=600)\n",
    "scatter = fig.data[0]\n",
    "scatter.on_click(handle_click)\n",
    "scatter.marker.size = [6] * len(df)\n",
    "scatter.marker.color = colors\n",
    "\n",
    "HBox([fig, out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
